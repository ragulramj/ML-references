{"cells":[{"cell_type":"markdown","source":["#### This is a Databricks Notebook for Data Preparation & Feature Extraction for POC Experimentation\n","In the inital phase of developing a ML solution, there is a need for experimentation before determining the best approach to develop the ML solution. This Notebook provides a framework to help with creating an intial snapshot of the historical training data to be used for POC experimentation. <br>\n","\n","**Input(s)**: Various data sources from Datalake & Deltalake <br>\n","**Outputs(s)**: Consolidated dataset to be saved back to Datalake<br>\n","**Recommended Cluster** : DBS_MAIN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"641a9741-f53d-46ef-ba22-f3a1ba883420"}}},{"cell_type":"markdown","source":["#### Step 1: Installing the required libraries.\n","* This step installs the required libraries for during development. <br/>\n","* Databricks' clusters has got commonly used libraries pre-installed. This is to install additional ones or specify specific versions of libraries. <br/>\n","* It is not recommended to install libraries at the cluster level as this would cause clashes for others\n","* dbutils.library.installPyPI(\"*pkg-name*\", version=\"*x.xx.x*\") \n","* This is used to install packages, version number is optional but reccommended."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de61ac0b-a7da-4ad9-8bcd-28842ecd9076"}}},{"cell_type":"code","source":["dbutils.library.installPyPI(\"pandas\", version=\"0.23.4\") ## Example\n","dbutils.library.installPyPI(\"scikit-learn\", version=\"0.21.3\")\n","\n","dbutils.library.restartPython() ## Restarting the Python shell, will clear all variables\n","print(dbutils.library.list())   ## Printing the list of packages "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4b2e7f7-5970-41d4-8bdf-33b6005202c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">PyPI package pandas has been installed already. The previously installed package is `pandas==0.23.4`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package scikit-learn has been installed already. The previously installed package is `scikit-learn==0.21.3`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\n[&#39;pandas==0.23.4&#39;, &#39;scikit-learn==0.21.3&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">PyPI package pandas has been installed already. The previously installed package is `pandas==0.23.4`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\nPyPI package scikit-learn has been installed already. The previously installed package is `scikit-learn==0.21.3`. To resolve this issue, detach and re-attach the notebook to create a new environment or rename the package.\n[&#39;pandas==0.23.4&#39;, &#39;scikit-learn==0.21.3&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2.1: Importing requried libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"678d46d8-79f1-4a33-841c-5f9c1287ae83"}}},{"cell_type":"code","source":["import pandas as pd\n","import glob, os\n","from datetime import datetime, timedelta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a3edfc0-8dac-4b8c-8bd8-c1ea95de59f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2.2: Accessing Key Vault\n","* It is important to not store passwords, API Keys, Credentials, Secrets, etc. as hard coded text in the code. This is mainly for security reasons and also for better maintenance.\n","* All the required confidential information should be stored in the keyvault  \n","* The confidential information can then be retrived via databricks using the following code.\n","* dbutils.secrets.get(scope = \"KEY_VAULT\", key = \"*SecretName*\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5686c8a0-8172-4211-a67e-86c8b6312c21"}}},{"cell_type":"code","source":["#Scope is required for all dbutils call to get secrets.\n","akv_scope =\"KEY_VAULT\"\n","\n","dbutils.secrets.get(scope = akv_scope, key = \"keyname\") ## replace key with the name of the secret within the key vault"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7892e88d-7e55-4db6-bf71-c460b150b6c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: &#39;[REDACTED]&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &#39;[REDACTED]&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["######################### Start of usecase specific codes #########################\n","## * Assumption is that the datalake is already mounted on databricks\n","\n","Data_path = '/dbfs/mnt/path_in_datalake' ## Path where raw data is stored \n","Experiment_path = '/dbfs/mnt/path_in_datalake to store experiment'\n","\n","## Reading from gen 1 datalake (using pandas)\n","filelist = glob.glob(Data_path + \"/*_full.csv\") ## analysing only specific files with full keyword\n","Merged_df = []\n","for csv in filelist:\n","  month_df = pd.read_csv(csv)\n","  Merged_df.append(month_df)\n","Merged_df = pd.concat(Merged_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb243ae-f2ad-48ff-9da6-e3510f44302a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3.2: Reading Data from DeltaLake\n","* Essentially, the deltalake is just a folder within the datalake which stores data in a table format. \n","* This allows for much an easier/logical data reading process compared to looping through multuple csv as is the case for conventional file based datalake\n","* Deltalake allows for SQL-like filtering and selecting commands using pyspark to return a subset of requried data.\n","* The deltalake resides/starts in the path which contains a *\"_delta_log\"* folder. <br>\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"241d9004-e6c3-483a-971f-6919a78961c5"}}},{"cell_type":"code","source":["### Loading the deltalake path into pyspark. All paths used are sample paths\n","spark_df = spark.read.format(\"delta\").load('/mnt/INDOD')\n","\n","### Applying filtering actions on the spark dataframe\n","filtered_spark_df = spark_df.filter(spark_df['src_timestamp_utc'] >= datetime.utcnow() - timedelta(days=10)) \\\n","#                             .filter(spark_df['gen_settlement_period'].isin(['12','13','14'])) \\\n","#                             .filter(spark_df['gen_settlement_period'] == '12') \n","\n","### Cnverting/Persisting spark df into pandas dataframe\n","PandasDF = filtered_spark_df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"774f6318-e9b3-4496-9f05-3d108103fa66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4: Start of Feature Engineering/Extraction methods\n","* In this step we will apply use-case specific feature engineering methods."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43257ddc-b590-49b4-83c2-0e368f0d33fd"}}},{"cell_type":"code","source":["### Applying feature extraction methods\n","Merged_df.index = pd.to_datetime(Merged_df.SETTLEMENTDATE, format='%Y/%m/%d %H:%M:%S')\n","\n","Merged_df['Month'] = Merged_df.index.month\n","Merged_df['Day'] = Merged_df.index.day\n","Merged_df['Day_of_week'] = Merged_df.index.dayofweek\n","\n","Merged_df.drop('SETTLEMENTDATE',axis=1,inplace=True)\n","Merged_df = Merged_df.drop_duplicates()\n","Merged_df = Merged_df.resample('30T').ffill()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"106c1535-c0ad-4646-95d7-0229467a2856"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Step 5: Writing the consolidated dataset back into the data lake\n","* Based on the defined process, aggregated data will be written into the project specific curated folder in the datalake\n","* In this case the data will be written to ''/dbfs/mnt/dlgen2/curated/energy/data_science/experiments/au/training_demo/POC'' (Experiment path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f87aa61a-7d75-4286-b7aa-df8a476d9a0c"}}},{"cell_type":"code","source":["RunDate = datetime.utcnow() ## Initializing a run date to as to name the files accordingly\n","\n","## Check if folder is already created and write to it\n","try:\n","  os.makedirs(Experiment_path)\n","except:\n","  print('Folder already exists')\n","\n","#### It is easier to first split the train and test set first to accomodate for Azure AutoML later\n","from sklearn.model_selection import train_test_split\n","TrainDF, TestDF = train_test_split(Merged_df, test_size=0.2, shuffle=False)\n","\n","TrainDF.to_csv(Experiment_path + RunDate.strftime('training_data_%Y%m%d.csv'))\n","TestDF.to_csv(Experiment_path + RunDate.strftime('testing_data_%Y%m%d.csv'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66f774c0-61aa-42cc-b4b6-1a2d7589d1ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Folder already exists\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Folder already exists\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### For custom cross-validation in Azure AutoML\n","For AzureML Python SDK ver 1.6.0 or later, in order to use our own custom cross-validation methods, we need to create addition columns to indicate which rows to use as training and which to use as validation for each fold. Each column represents one cross-validation split, and is filled with integer values 1 or 0 --where 1 indicates the row should be used for training and 0 indicates the row should be used for validation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59c791a3-83a2-42a3-94a2-400316a5a2ca"}}},{"cell_type":"code","source":["from sklearn.model_selection import TimeSeriesSplit\n","cv = TimeSeriesSplit(n_splits=10)\n","\n","for i, (train_ind, valid_ind) in enumerate(cv.split(TrainDF)):\n","  TrainDF['cv'+str(i+1)] = 0\n","  ind = TrainDF.index[train_ind]\n","  TrainDF.loc[ind,'cv'+str(i+1)] = 1\n","  \n","TrainDF.to_csv(Experiment_path + RunDate.strftime('training_data_%Y%m%d.csv'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9cabb8c-a4e1-48bc-aeef-6a9f66e77f87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/local_disk0/tmp/1613700674811-0/PythonShell.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  #\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-ce346c89-d9ee-4e96-bd76-6b76e26e29c1/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1613700674811-0/PythonShell.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  #\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-ce346c89-d9ee-4e96-bd76-6b76e26e29c1/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb37cd64-308c-4cce-bf00-5a7b6cf191a9"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"POC_DataPrep","notebookId":4179428956476726,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"POC_DataPrep","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4179428956480000}},"nbformat":4,"nbformat_minor":0}